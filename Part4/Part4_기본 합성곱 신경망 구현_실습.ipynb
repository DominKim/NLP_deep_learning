{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 합성곱 신경망 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"True\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "\n",
    "batch_size = 100\n",
    "no_cuda = False\n",
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "torch.manual_seed(777)\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, padding = 1), # 28 x 28 x 16\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 14 x 14 x 32\n",
    "            nn.Conv2d(16, 32, 3, padding = 1), # 14 x 14 x 32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 7 x 7 x 32\n",
    "            nn.Conv2d(32, 64, 3, padding = 1), # 7 x 7 x 64\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding = 1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2) # 7 x 7 x 64\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(576, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 10)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        out = out.view(out.size(0), -1) # \n",
    "        out = self.fc(out)\n",
    "        return F.softmax(out, dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습, 테스트 루프 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\"../data\", train = True, download = True,\n",
    "                     transform = transforms.Compose([\n",
    "                         transforms.ToTensor()\n",
    "                     ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "\n",
    "test_data = datasets.MNIST(\"../data\", train = False, download = True,\n",
    "                     transform = transforms.Compose([\n",
    "                         transforms.ToTensor()\n",
    "                     ]))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 환경 정의\n",
    "### 모델 생성, 손실함수, 최적화 알고리즘, 평가지표 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create model\n",
    "# model = ConvNet()\n",
    "\n",
    "# # Define loss and optimizer\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# # Define performance metrics\n",
    "# train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "# train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "# test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "# test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# factor는 기존 값 * factor\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode = \"max\", factor = 0.1, \n",
    "                              patience=0, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302680\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.361151\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.381151\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.361151\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.351151\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.401151\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.391151\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.341151\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.381151\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.361151\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.331151\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.381151\n",
      "Epoch     2: reducing learning rate of group 0 to 1.0000e-02.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.301151\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.321151\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.391151\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.341151\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.371151\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 2.381151\n",
      "Epoch     3: reducing learning rate of group 0 to 1.0000e-03.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.331151\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 2.331151\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 2.381151\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 2.371151\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 2.421151\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 2.351151\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-04.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.351151\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.361151\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 2.381151\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.381151\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 2.361151\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 2.411151\n",
      "Epoch     5: reducing learning rate of group 0 to 1.0000e-05.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.351151\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 2.391151\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 2.391151\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 2.341151\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 2.371151\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 2.391151\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-06.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.361151\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 2.381151\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 2.421151\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 2.281151\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 2.391151\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 2.311151\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-07.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.301151\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 2.371151\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 2.391151\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 2.361151\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 2.371151\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 2.331151\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-08.\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.341151\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 2.351151\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 2.331151\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 2.351151\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 2.381151\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 2.361151\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.391151\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 2.411151\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 2.321151\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 2.341151\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 2.401151\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 2.361151\n",
      "\n",
      "Test set: Average loss: 2.3638, Accuracy: 974/10000 (10%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    # Train Mode\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)  # https://pytorch.org/docs/stable/nn.html#nll-loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "    \n",
    "    # Test mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    accuracy = 100 * correct / len(test_loader.dataset)\n",
    "\n",
    "    # Learning Rate Shecduler\n",
    "    scheduler.step(accuracy, epoch)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
